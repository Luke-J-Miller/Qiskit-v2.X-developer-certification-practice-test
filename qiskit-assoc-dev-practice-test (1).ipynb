{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13301240,"sourceType":"datasetVersion","datasetId":8426090},{"sourceId":266445321,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"BLANK_STUDENT_HISTORY_FILE = \"/kaggle/input/qiskit-associate-developer-cert-practice/student_history.pkl\"\nSTUDENT_HISTORY_SAVE_FOLDER = \"/kaggle/working/\"\nTASK_BY_SECTION_DICT_FILE = \"/kaggle/input/qiskit-associate-developer-cert-practice/task_by_section_dict.pkl\"\nPERCENT_BY_TASK_DICT_FILE = \"/kaggle/input/qiskit-associate-developer-cert-practice/percentage_by_task_dict.pkl\"\nQUESTION_BANK_DF_FILE = \"/kaggle/input/qiskit-associate-developer-cert-practice/question_df.pkl\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T16:25:48.538193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\nimport pickle\nfrom datetime import datetime\nimport pickle\ndef save_student_history(student_history):\n    root = STUDENT_HISTORY_SAVE_FOLDER\n    # Make the filename FS-friendly\n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    save_path = root + f\"student_history_{ts}.pkl\"\n    # ✅ use 'wb' (write-binary), not 'rb'\n    with open(save_path, 'wb') as f:\n        pickle.dump(student_history, f, protocol=pickle.HIGHEST_PROTOCOL)\n    print(\"Student History saved to:\", save_path)\n\ndef update_scores(student_history):\n    \"\"\"Vectorized 'score' = (Number_correct + 1) / (Number_attempts*(1+eps) + eps).\"\"\"\n    import numpy as np\n    epsilon = 1e-3\n    attempt_penalty = 1 + epsilon\n    num = student_history[\"Number_correct\"].astype(float) + 1.0\n    den = student_history[\"Number_attempts\"].astype(float) * attempt_penalty + epsilon\n    student_history[\"score\"] = num / den  # ✅ assign back to the DF (no .iterrows mutation bug)\n\ndef load_student_history(file_path):\n    \"\"\"Returns (student_history, resolved: bool). If 'None', create a new empty structure.\"\"\"\n    import pandas as pd\n    if file_path in [\"None\", \"none\", \"NONE\"]:\n        print(\"Creating new student history (empty).\")\n        # Caller typically initializes from question_df; return placeholder here.\n        file_path = BLANK_STUDENT_HISTORY_FILE\n    try:\n        with open(file_path, \"rb\") as f:\n            student_history = pickle.load(f)\n        print(\"Loaded Student History\")\n        # Ensure required columns exist\n        \n        return student_history, True\n    except Exception as e:\n        print(f\"Unable to load student history: {e}\")\n        return None, False\n    for c in [\"Number_attempts\",\"Number_correct\"]:\n        if c not in student_history.columns:\n            student_history[c] = 0\n        if \"score\" not in student_history.columns:\n            update_scores(student_history)\n    return student_history, True\nimport pickle\nimport sys\nfrom pathlib import Path\nimport pandas as pd\n\n# ---------------------------\n# Safe loaders and utilities\n# ---------------------------\n\ndef safe_load_pickle(path_str, required=True, expected_type=None, var_name=\"object\"):\n    p = Path(path_str)\n    if not p.exists():\n        msg = f\"[ERROR] Missing file: {p}\"\n        if required:\n            raise FileNotFoundError(msg)\n        else:\n            print(msg)\n            return None\n    if p.stat().st_size == 0:\n        msg = f\"[ERROR] Empty file: {p}\"\n        if required:\n            raise ValueError(msg)\n        else:\n            print(msg)\n            return None\n    with open(p, \"rb\") as f:\n        obj = pickle.load(f)\n    if expected_type is not None and not isinstance(obj, expected_type):\n        msg = f\"[ERROR] {var_name} should be {expected_type}, got {type(obj)} from {p}\"\n        if required:\n            raise TypeError(msg)\n        else:\n            print(msg)\n            return None\n    return obj\n\ndef prompt_choice(prompt, valid_choices, to_lower=True):\n    \"\"\"Prompt until user enters a valid string choice from valid_choices.\"\"\"\n    valid = {str(v).lower() if to_lower else str(v): v for v in valid_choices}\n    while True:\n        val = input(prompt).strip()\n        key = val.lower() if to_lower else val\n        if key in valid:\n            return valid[key]\n        print(f\"Invalid selection. Valid options: {sorted(valid_choices)}\")\n\ndef prompt_int(prompt, min_val=None, max_val=None):\n    \"\"\"Prompt until user enters an int within optional [min_val, max_val].\"\"\"\n    while True:\n        val = input(prompt).strip()\n        try:\n            n = int(val)\n        except ValueError:\n            print(\"Please enter a whole number.\")\n            continue\n        if min_val is not None and n < min_val:\n            print(f\"Enter a number >= {min_val}.\")\n            continue\n        if max_val is not None and n > max_val:\n            print(f\"Enter a number <= {max_val}.\")\n            continue\n        return n\n\ndef prompt_yes_no(prompt):\n    \"\"\"Return True/False from 1/0 (or y/n).\"\"\"\n    while True:\n        val = input(prompt + \" \").strip().lower()\n        if val in {\"1\", \"y\", \"yes\"}:\n            return True\n        if val in {\"0\", \"n\", \"no\"}:\n            return False\n        print(\"Please enter 1/0 or y/n.\")\n\n# ---------------------------\n# Data loads (protected)\n# ---------------------------\n\ntry:\n    file_path = TASK_BY_SECTION_DICT_FILE\n    task_by_section_dict = safe_load_pickle(file_path,\n                                            required=True, expected_type=dict,\n                                            var_name=\"task_by_section_dict\")\n    file_path = PERCENT_BY_TASK_DICT_FILE\n    percentage_by_task_dict = safe_load_pickle(file_path,\n                                               required=True, expected_type=dict,\n                                               var_name=\"percentage_by_task_dict\")\n    file_path = QUESTION_BANK_DF_FILE\n    question_df = safe_load_pickle(file_path,required=True, expected_type=pd.DataFrame,var_name=\"question_df\")\n    # AFTER (minimal fix)\n    question_df = question_df[question_df[\"Question\"].astype(str).str.strip().str.lower() != 'question']\n    question_df = question_df.sample(frac=1).reset_index(drop=True)\n\n        \nexcept Exception as e:\n    print(e)\n    sys.exit(1)\n\n# Basic sanity checks\nrequired_cols = {\"Question\", \"Section\", \"Task\"}\nmissing = required_cols - set(question_df.columns)\nif missing:\n    print(f\"[ERROR] question_df missing required columns: {missing}\")\n    sys.exit(1)\n\nif len(question_df) == 0:\n    print(\"[ERROR] question_df is empty; cannot proceed.\")\n    sys.exit(1)\n\n# ---------------------------\n# Stats placeholder\n# ---------------------------\n\ndef print_stats(student_history: pd.DataFrame):\n    import pandas as pd\n\n    if not isinstance(student_history, pd.DataFrame):\n        print(\"[ERROR] student_history is not a DataFrame.\")\n        return\n\n    needed = {\"Question\", \"Section\", \"Task\", \"Number_attempts\", \"Number_correct\"}\n    miss = needed - set(student_history.columns)\n    if miss:\n        print(f\"[ERROR] student_history missing columns: {miss}\")\n        return\n\n    # Overall metrics\n    total_questions = len(student_history)\n    ever_attempted = int((student_history[\"Number_attempts\"] > 0).sum())\n    total_attempts = int(student_history[\"Number_attempts\"].sum())\n    total_correct  = int(student_history[\"Number_correct\"].sum())\n    overall_acc = (100.0 * total_correct / total_attempts) if total_attempts > 0 else 0.0\n\n    print(\"=== Student History — Overall ===\")\n    print(f\"Questions ever attempted : {ever_attempted} / {total_questions}\")\n    print(f\"Total attempts           : {total_attempts}\")\n    print(f\"Total correct            : {total_correct}\")\n    print(f\"Overall accuracy         : {overall_acc:.2f}%\")\n\n    # By-section aggregation\n    grp = student_history.groupby(\"Section\", dropna=False, as_index=False).agg(\n        total_questions=(\"Question\", \"count\"),\n        ever_attempted=(\"Number_attempts\", lambda s: int((s > 0).sum())),\n        attempts=(\"Number_attempts\", \"sum\"),\n        correct=(\"Number_correct\", \"sum\"),\n    )\n    # Section accuracy = sum(correct)/sum(attempts) for that section\n    grp[\"accuracy_pct\"] = grp.apply(\n        lambda r: (100.0 * r[\"correct\"] / r[\"attempts\"]) if r[\"attempts\"] > 0 else 0.0,\n        axis=1\n    )\n\n    if not grp.empty:\n        print(\"\\n=== By Section ===\")\n        # Pretty print a compact table\n        cols = [\"Section\", \"ever_attempted\", \"total_questions\", \"attempts\", \"correct\", \"accuracy_pct\"]\n        # Align and format\n        for _, r in grp[cols].sort_values(\"Section\").iterrows():\n            print(f\"- {r['Section']}: \",\n                f\"attempted {int(r['ever_attempted'])}/{int(r['total_questions'])} | \",\n                f\"attempts {int(r['attempts'])} | correct {int(r['correct'])} | \",\n                f\"acc {r['accuracy_pct']:.2f}%\")\n\n\n# ---------------------------\n# Menu and parameter prompts\n# ---------------------------\n\nTOPIC_MAP = {\n    \"1\": 'Section 1: Perform quantum operations',\n    \"2\": 'Section 2: Visualize quantum circuits, measurements, and states',\n    \"3\": 'Section 3: Create quantum circuits',\n    \"4\": 'Section 4: Run quantum circuits',\n    \"5\": 'Section 5: Use the sampler primitive',\n    \"6\": 'Section 6: Use the estimator primitive',\n    \"7\": 'Section 7: Retrieve and analyze the results of quantum circuits',\n    \"8\": 'Section 8: Operate with OpenQASM'\n}\n\ndef main_menu(student_history: pd.DataFrame):\n    \"\"\"\n    Returns (quit_flag: bool, practice_flag: bool)\n    \"\"\"\n    print(\"\"\"\nWhat would you like to do (type only the number)\n    1 Take a practice test\n    2 Save student history\n    3 Quit\n    4 View stats on my student history\n\"\"\")\n    choice = prompt_choice(\"Enter choice: \", {\"1\", \"2\", \"3\", \"4\"})\n    if choice == \"1\":\n        return (False, True)\n    elif choice == \"2\":\n        save_student_history(student_history)\n        return (False, False)\n    elif choice == \"3\":\n        return (True, False)\n    else:  # \"4\"\n        print_stats(student_history)\n        return (False, False)\n\ndef get_practice_params(question_df: pd.DataFrame):\n    \"\"\"\n    Returns:\n      full_test: bool  (True = exam over all topics; False = select topics)\n      number_of_questions: int\n      timed: bool\n      adaptive_mode: int  (0, 1, or 2)\n      quit_requested: bool\n      topic_key: str or None (if not selecting by topic)\n    \"\"\"\n    total_questions = len(question_df)\n    if total_questions <= 0:\n        print(\"[ERROR] No questions available.\")\n        return True, 0, False, 2, True, None\n\n    print(\"\"\"Type '0' for an exam over all topics (proportions roughly match the test).\nType '1' to select a topic.\"\"\")\n    ft_choice = prompt_choice(\"Enter 0 or 1: \", {\"0\", \"1\"})\n    full_test = (ft_choice == \"0\")\n\n    # Number of questions (bound to available)\n    number_of_questions = prompt_int(\n        f\"How many questions would you like? (1..{total_questions}) \",\n        min_val=1,\n        max_val=total_questions)\n\n    # Timing suggestion\n    suggested_seconds = int(round(79.411 * number_of_questions))\n    timed = prompt_yes_no(\n        f\"Do you want your test timed? Suggested {suggested_seconds} seconds for {number_of_questions} questions. (1=yes, 0=no)\")\n\n    # Adaptive mode\n    print(\"\"\"Adaptive mode:\n  0: Focus on questions you haven't seen (if all seen -> least seen; if none seen -> random)\n  1: Focus on questions you got wrong the most (if none seen -> random)\n  2: Random\n\"\"\")\n    adaptive_mode = prompt_choice(\"Enter 0, 1, or 2: \", {\"0\", \"1\", \"2\"})\n    adaptive_mode = int(adaptive_mode)\n\n    quit_requested = False\n    topic_key = None\n\n    if not full_test:\n        # Topic selection loop\n        while True:\n            print(\"Select a topic (or 0 to quit):\")\n            for k, v in TOPIC_MAP.items():\n                print(f\"  {k}: {v}\")\n            t = prompt_choice(\"Enter number (0..8): \", set(TOPIC_MAP.keys()) | {\"0\"})\n            if t == \"0\":\n                quit_requested = True\n                break\n            topic_key = t\n            break  # valid topic chosen\n\n    return full_test, number_of_questions, timed, adaptive_mode, quit_requested, topic_key\n\n# ---------------------------\n# Example student_history init (if not already loaded)\n# ---------------------------\n\n# If you already have a student_history loaded elsewhere, skip this block.\nif 'student_history' not in globals():\n    base = question_df[[\"Question\", \"Section\", \"Task\"]].copy()\n    # If the same Question appears in multiple tasks/sections and you want them separate, keep duplicates.\n    # Otherwise: base = base.drop_duplicates().reset_index(drop=True)\n    student_history = base.copy()\n    student_history[\"Number_attempts\"] = 0\n    student_history[\"Number_correct\"] = 0\n    # Optional: stable ids\n    student_history.insert(0, \"Question_ID\", range(1, len(student_history) + 1))\nimport pandas as pd\n\ndef get_topic_allocation(number_questions: int, percentage_by_task_dict: dict) -> dict:\n    \"\"\"\n    Integer allocation per task whose sum equals number_questions,\n    using largest-remainder (Hamilton) rounding on percentages.\n    \"\"\"\n    # raw (float) allocations\n    raw = {\n        task: percentage_by_task_dict[task] * number_questions / 100.0\n        for task in percentage_by_task_dict}\n    # floors\n    alloc = {task: int(v) for task, v in raw.items()}\n    used = sum(alloc.values())\n    remaining = number_questions - used\n    if remaining <= 0:\n        return alloc\n\n    # distribute by largest fractional remainders\n    remainders = sorted(\n        ((task, raw[task] - alloc[task]) for task in percentage_by_task_dict),\n        key=lambda x: x[1],\n        reverse=True)\n    for i in range(remaining):\n        alloc[remainders[i % len(remainders)][0]] += 1\n\n    return alloc\n\n\ndef generate_test(full_test: bool, \n                  n_questions: int, \n                  timed: bool, \n                  adaptive_mode: int, \n                  want_quit: bool, \n                  topic_key: str,\n                  student_history: pd.DataFrame):\n    \"\"\"\n    Returns (practice_test_df, test_duration_seconds).\n    Uses student_history to prioritize selections for adaptive modes.\n    \"\"\"\n    # guard\n    required_q_cols = {\"Question\", \"Section\", \"Task\"}\n    required_h_cols = {\"Question\", \"Section\", \"Task\", \"Number_attempts\", \"Number_correct\"}\n    if not required_q_cols.issubset(set(question_df.columns)):\n        raise ValueError(\"question_df missing required columns\")\n    if not required_h_cols.issubset(set(student_history.columns)):\n        raise ValueError(\"student_history missing required columns\")\n\n    test_duration = n_questions * 79.411  # seconds (suggested)\n\n    if full_test:\n        question_bank = question_df.copy()\n        topic_question_counts = get_topic_allocation(n_questions, percentage_by_task_dict)\n    else:\n        topic = TOPIC_MAP[topic_key]\n        question_bank = question_df[question_df[\"Section\"] == topic].copy()\n\n        tasks_in_topic = list(task_by_section_dict.get(topic, []))\n        if len(tasks_in_topic) == 0:\n            # fallback: treat entire section as one bucket\n            topic_question_counts = {None: n_questions}\n        else:\n            base = n_questions // len(tasks_in_topic)\n            topic_question_counts = {t: base for t in tasks_in_topic}\n            # distribute remainder\n            rem = n_questions - sum(topic_question_counts.values())\n            for i in range(rem):\n                topic_question_counts[tasks_in_topic[i % len(tasks_in_topic)]] += 1\n\n    def select_by_history_for_task(task_name: str, k: int, mode: int) -> pd.DataFrame:\n        \"\"\"Pick up to k rows from question_bank for a given task using student_history priority.\"\"\"\n        if task_name is None:\n            hist_slice = student_history.copy()\n            bank_slice = question_bank.copy()\n        else:\n            hist_slice = student_history[student_history[\"Task\"] == task_name]\n            bank_slice = question_bank[question_bank[\"Task\"] == task_name]\n\n        if mode == 0:  # unseen / least seen first\n            hist_slice = hist_slice.sort_values(by=\"Number_attempts\", ascending=True)\n        elif mode == 1:  # most wrong (lowest score first)\n            # ensure score exists; if not, compute quickly\n            if \"score\" not in hist_slice.columns:\n                # (Number_correct + 1) / (Number_attempts*(1+eps) + eps)\n                eps = 1e-3\n                attempt_penalty = 1.0 + eps\n                num = hist_slice[\"Number_correct\"].astype(float) + 1.0\n                den = hist_slice[\"Number_attempts\"].astype(float) * attempt_penalty + eps\n                hist_slice = hist_slice.copy()\n                hist_slice[\"score\"] = num / den\n            hist_slice = hist_slice.sort_values(by=\"score\", ascending=True)\n        else:\n            # random: sample directly from bank\n            return bank_slice.sample(n=min(k, len(bank_slice)))\n\n        # Map prioritized questions to rows in bank\n        prioritized_qs = hist_slice[\"Question\"].tolist()\n        # keep order with Categorical sort, then take top k\n        if len(prioritized_qs) == 0:\n            return bank_slice.sample(n=min(k, len(bank_slice)))\n\n        ordered = (bank_slice.assign(_ord=pd.Categorical(\n            bank_slice[\"Question\"], \n            categories=prioritized_qs, ordered=True)).sort_values(\n            \"_ord\", na_position=\"last\").drop(columns=[\"_ord\"]))\n        return ordered.head(k)\n\n    practice_parts = []\n    if full_test:\n        for task, k in topic_question_counts.items():\n            if k <= 0:\n                continue\n            bank_slice = question_bank[question_bank[\"Task\"] == task]\n            if len(bank_slice) == 0:\n                continue\n            take = select_by_history_for_task(task, k, adaptive_mode)\n            practice_parts.append(take)\n    else:\n        # in-section selection across tasks per topic_question_counts\n        for task, k in topic_question_counts.items():\n            if k <= 0:\n                continue\n            if task is None:\n                bank_slice = question_bank\n            else:\n                bank_slice = question_bank[question_bank[\"Task\"] == task]\n            if len(bank_slice) == 0:\n                continue\n            take = select_by_history_for_task(task, k, adaptive_mode)\n            practice_parts.append(take)\n\n    practice_test = pd.concat(practice_parts, ignore_index=True) if practice_parts else pd.DataFrame(columns=list(question_df.columns))\n\n    # If we somehow undershot (e.g., not enough items in some buckets), top up randomly from remaining pool\n    if len(practice_test) < n_questions and len(question_bank) > len(practice_test):\n        remaining_needed = n_questions - len(practice_test)\n        remaining_pool = question_bank.merge(practice_test[[\"Question\"]], on=\"Question\", how=\"left\", indicator=True)\n        remaining_pool = remaining_pool[remaining_pool[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n        if len(remaining_pool) > 0:\n            practice_test = pd.concat([practice_test, remaining_pool.sample(n=min(remaining_needed, len(remaining_pool)), random_state=42)],ignore_index=True)\n\n    # final cap in case of over-selection\n    if len(practice_test) > n_questions:\n        practice_test = practice_test.sample(n=n_questions).reset_index(drop=True)\n    else:\n        practice_test = practice_test.reset_index(drop=True)\n\n    return practice_test, test_duration\n\nresolved = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# ------- load (unchanged) -------\nwhile not resolved:\n    file_path = input(\"Provide file path for student history file.  If you don't want to load a student history, type 'None' \")\n    student_history, resolved = load_student_history(file_path)\n\n# ------- main loop -------\nquit_flag = False\nwhile not quit_flag:\n    quit_flag, practice_flag = main_menu(student_history)\n    if quit_flag:\n        break\n    if practice_flag:\n        full_test, n_questions, timed, adaptive_mode, want_quit, topic_key = get_practice_params(question_df)\n        if want_quit:\n            continue\n\n        print(\"\\n=== Practice Parameters ===\")\n        print(f\"Full test?         : {full_test}\")\n        print(f\"Num questions      : {n_questions}\")\n        print(f\"Timed              : {timed}\")\n        print(f\"Adaptive mode      : {adaptive_mode}\")\n        print(f\"Topic              : {TOPIC_MAP.get(topic_key, 'All topics')}\")\n        print(\"===========================\\n\")\n\n        practice_test, duration = generate_test(\n            full_test,\n            n_questions, \n            timed, \n            adaptive_mode, \n            want_quit, \n            topic_key,\n            student_history\n        )\n\n        start_time = time.time()\n        if timed:\n            print(f\"You have {duration:.0f} seconds — about {int(duration//60)} minutes.\")\n\n        correct_count = 0\n\n        for _, row in practice_test.iterrows():\n            print(\"\\nQuestion\")\n            print(row[\"Question\"])\n            print(\"A:\", row.get(\"Choice_A\", \"\"))\n            print(\"B:\", row.get(\"Choice_B\", \"\"))\n            print(\"C:\", row.get(\"Choice_C\", \"\"))\n            print(\"D:\", row.get(\"Choice_D\", \"\"))\n\n            answer = input(\"Select your choice: A, B, C, or D \").strip().upper()[:1]\n\n            # update attempts\n            mask = (student_history[\"Question\"] == row[\"Question\"])\n            student_history.loc[mask, \"Number_attempts\"] = student_history.loc[mask, \"Number_attempts\"] + 1\n\n            # time check\n            current_duration = time.time() - start_time\n            if timed:\n                remaining = max(0, duration - current_duration)\n                print(f\"Time remaining = {remaining:.1f} seconds\")\n                if current_duration > duration:\n                    print(\"Out of time.\")\n                    break\n\n            # evaluate answer\n            is_valid = answer in {\"A\", \"B\", \"C\", \"D\"}\n            correct_field = row.get(\"Correct_Answer\", None)\n\n            if not is_valid:\n                print(\"You entered an invalid answer choice. Question marked wrong.\")\n                is_correct = False\n            else:\n                # If Correct_Answer is a letter, compare letters; otherwise compare the text of the chosen option.\n                if isinstance(correct_field, str) and correct_field in {\"A\", \"B\", \"C\", \"D\"}:\n                    is_correct = (answer == correct_field)\n                else:\n                    is_correct = (row.get(f\"Choice_{answer}\") == correct_field)\n\n            if is_correct:\n                print(\"Correct!\")\n                correct_count += 1\n                student_history.loc[mask, \"Number_correct\"] = student_history.loc[mask, \"Number_correct\"] + 1\n            else:\n                print(\"Incorrect. The correct answer is:\")\n                if isinstance(correct_field, str) and correct_field in {\"A\", \"B\", \"C\", \"D\"}:\n                    print(correct_field)\n                else:\n                    print(correct_field)\n\n            # explanation (if present)\n            expl = row.get(\"Explanation\", None)\n            if expl is not None:\n                print(\"Explanation:\")\n                print(expl)\n\n        total_time = time.time() - start_time\n        print(f\"\\nYou got {correct_count} of {n_questions} correct in {total_time:.1f} seconds.\")\n\n        update_scores(student_history)\n        print_stats(student_history)\n        save_student_history(student_history)","metadata":{"trusted":true},"outputs":[{"output_type":"stream","name":"stdin","text":"Provide file path for student history file.  If you don't want to load a student history, type 'None'  /kaggle/working/student_history_20251008_155901.pkl\nEnter choice:  quit\n"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}